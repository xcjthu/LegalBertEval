[train] #train parameters
epoch = 50
batch_size = 2

shuffle = True

reader_num = 32

optimizer = adamw
learning_rate = 1e-5
step_size = 1
lr_multiplier = 1

PLM_vocab = hfl/chinese-roberta-wwm-ext
PLM_path = thunlp/Lawformer

max_len = 2560

grad_accumulate = 4

[distributed]
use = True
backend = nccl

[eval] #eval parameters
batch_size = 2

shuffle = False

reader_num = 16

[data] #data parameters
train_dataset_type = law
train_formatter_type = law
train_data_path = /home/ubuntu/mnt/LawPrediction/LawPrediction/data/final_data/train.json


valid_dataset_type = law
valid_formatter_type = law
valid_data_path = /home/ubuntu/mnt/LawPrediction/LawPrediction/data/final_data/test.json


label2id = /home/ubuntu/mnt/LawPrediction/LawPrediction/data/final_data/label2id.json

[model] #model parameters
model_name = law


[output] #output parameters
model_path = /home/ubuntu/mnt/LawPrediction/LawPrediction/model
model_name = law

tensorboard_path = /home/ubuntu/mnt/LawPrediction/LawPrediction/tensorboard

output_function = Basic
output_value = mip,mir,mif,map,mar,maf
